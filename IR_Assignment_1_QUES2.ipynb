{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Assignment_1_QUES2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD6kNqcZAlH0",
        "outputId": "3ac6a7bc-db01-4ecf-bcdf-04b7882a0af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import List,DefaultDict,Dict\n",
        "from collections import defaultdict\n",
        "import pathlib\n",
        "import nltk\n",
        "from nltk.tokenize import punkt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "ioea0SLpAp0O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass \n",
        "class TermDetails:\n",
        "  doc_id:str\n",
        "  positions:List[str]\n",
        "  term_frequency:int=0\n"
      ],
      "metadata": {
        "id": "IOlIuT3cB_G2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### CREATING DATA STRUCTURE #####\n",
        "positional_index:DefaultDict[str,Dict[str,TermDetails]]=defaultdict(dict)"
      ],
      "metadata": {
        "id": "zK1sjNNcKO6G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YqpO5t7uwAJ",
        "outputId": "bb0cfcc9-e49b-41f5-b718-68ed651f9649"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.66-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 65.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.66 pyahocorasick-1.4.4 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JGV2N7bwJZK",
        "outputId": "1f15599e-6549-4412-e8f2-f9bf53badbe4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaFbNH9wwhZ6",
        "outputId": "d5815942-52f9-4499-a1cf-a82d60c81e69"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRATL6Mawl_L",
        "outputId": "74d44aec-b74e-4059-a40a-a0eada7eb641"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string, unicodedata\n",
        "import nltk\n",
        "import contractions\n",
        "import inflect\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "9eIAL_CCtBHg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### PRE PROCESSING OF THE TEXT #####\n",
        "\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "def replace_contractions(text):\n",
        "    \"\"\"Replace contractions in string of text\"\"\"\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_underscores(words):\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "      new_word = re.sub(r'_{2,}', '', word)\n",
        "      if new_word != '':\n",
        "          new_words.append(new_word)\n",
        "  return new_words\n",
        "\n",
        "\n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "\n",
        "        if word.isdigit():\n",
        "          try:\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "          except:\n",
        "            pass\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = replace_numbers(words)\n",
        "    words = remove_stopwords(words)\n",
        "    words = remove_underscores(words)\n",
        "    return words\n",
        "  \n",
        "def stem_and_lemmatize(words):\n",
        "    stems = stem_words(words)\n",
        "    lemmas = lemmatize_verbs(words)\n",
        "    return stems, lemmas"
      ],
      "metadata": {
        "id": "1SGmlmFYs1gR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### CREATING DATA STRUCTURE #####\n",
        "current_directory=pathlib.Path(os.path.abspath(\"\"))\n",
        "path=current_directory / pathlib.Path(\"/content/drive/MyDrive/IR_A1_Dataset\")\n",
        "for filename in path.glob(\"**/*\"):\n",
        "  if not filename.is_file():\n",
        "    continue\n",
        "  with filename.open('rb') as f:\n",
        "    doc_id=filename.name\n",
        "    raw_text = f.read()\n",
        "    raw_text = denoise_text(raw_text)\n",
        "    raw_text = replace_contractions(raw_text)\n",
        "    words = nltk.word_tokenize(raw_text)\n",
        "    words = normalize(words)\n",
        "    stems, lemmas = stem_and_lemmatize(words)\n",
        "    cleaned_text = \" \".join(lemmas)\n",
        "    for lemma in lemmas:\n",
        "      positions = []\n",
        "      for match in re.finditer(lemma,cleaned_text):\n",
        "        positions.append(match.start())\n",
        "      if lemma not in positional_index:\n",
        "\n",
        "        positional_index[lemma]={doc_id:TermDetails(doc_id=doc_id, positions=positions, term_frequency=len(positions))}\n",
        "      else:\n",
        "        if doc_id in positional_index[lemma]:\n",
        "          positional_index[lemma][doc_id].positions.extend(positions)\n",
        "          positional_index[lemma][doc_id].term_frequency += len(positions)\n",
        "        else:\n",
        "          positional_index[lemma][doc_id] = TermDetails(doc_id=doc_id, positions=positions, term_frequency=len(positions))\n",
        "    # print(filename)\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9anVtxUGoQgq",
        "outputId": "40306c4c-877c-4bc3-8156-9e8282350100"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# with open('/content/drive/MyDrive/cleaned.json','w') as f:\n",
        "#   json.dump(positional_index,f,default=lambda o:o.__dict__)\n"
      ],
      "metadata": {
        "id": "4hZjVHC8FniU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FUNCTION TO FIND THE DOCUMENTS FROM THE DATA STRUCTURE ####\n",
        "def phrase_query(phrase):\n",
        "  starting_dict_keys=set(positional_index[phrase[0]].keys())\n",
        "  # print(starting_dict_keys)\n",
        "  # common_keys=copy.deepcopy()\n",
        "  for i in range(1,len(phrase)):\n",
        "    starting_dict_keys=set(positional_index[phrase[i]].keys()).intersection(set(starting_dict_keys))\n",
        "    # print(starting_dict_keys)\n",
        "    removed_keys=set()\n",
        "    for key in starting_dict_keys:\n",
        "      \n",
        "      for k in positional_index[phrase[i]][key].positions:\n",
        "        # print(k)\n",
        "        for j in positional_index[phrase[i-1]][key].positions:\n",
        "          # print(j,k)\n",
        "          if(k==j+len(phrase[i-1])+1):\n",
        "            # print(j)\n",
        "            k+=1\n",
        "            break\n",
        "        else:\n",
        "          continue\n",
        "        break\n",
        "      else:\n",
        "        removed_keys.add(key)\n",
        "    for el in removed_keys:\n",
        "      starting_dict_keys.remove(el)\n",
        "  print(\"number of documents retrieved: \",len(starting_dict_keys))\n",
        "  print(\"list of documents retrieved:\")    \n",
        "  print(starting_dict_keys)\n",
        "            \n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "MwVVedchKNNl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to search the string ####\n",
        "def searchDocs(string):\n",
        "  raw_text = denoise_text(string)\n",
        "\n",
        "  raw_text = replace_contractions(raw_text)\n",
        "\n",
        "\n",
        "  words = nltk.word_tokenize(raw_text)\n",
        "\n",
        "  words = normalize(words)\n",
        "  # print(words)\n",
        "  stems, lemmas = stem_and_lemmatize(words)\n",
        "  # print(lemmas)\n",
        "  phrase_query(lemmas)"
      ],
      "metadata": {
        "id": "qUPSetr_fDHO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##EXAMPLE###\n",
        "string = input(\"Input the string you want to search: \")\n",
        " \n",
        "searchDocs(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfJGXwjeeEGa",
        "outputId": "ed9e97d6-310a-44e0-81f3-b3cc3627065d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input the string you want to search: came out\n",
            "number of documents retrieved:  630\n",
            "list of documents retrieved:\n",
            "{'coffee.txt', 'miamadvi.hum', 'drive.txt', 'prac4.jok', 'a_tv_t-p.com', 'legal.hum', 'deadlysins.txt', 'mr.rogers', 'jason.fun', 'zen.txt', 'planeget.hum', 'bnb_quot.txt', 'soleleer.hum', 'inquirer.txt', 'byfb.txt', 'trekwes.hum', 'polly.txt', 'various.txt', 'nukeplay.hum', 'normquot.txt', 'mead.rcp', 'hackingcracking.txt', 'variety3.asc', 'hecomes.jok', 'sungenu.hum', 'variety1.asc', 'petshop', 'resolutn.txt', 'tfpoems.hum', 'mash.hum', 'jc-elvis.inf', 'number_k.ill', 'nigel.10', 'rinaldos.txt', 'gd_flybd.txt', 'fajitas.rcp', 'gack!.txt', 'japantv.txt', 'quantum.jok', 'stuf10.txt', 'dieter.txt', 'insult', 'nigel.4', 'aeonint.txt', 'hacktest.txt', 'bbc_vide.cat', 'terms.hum', 'golnar.txt', 'wkrp.epi', 'lobquad.hum', 'butwrong.hum', 'films_gl.txt', 'mindvox', 'passenge.sim', 'readme.bat', 'cybrtrsh.txt', 'htswfren.txt', 'melodram.hum', 'princess.brd', 'humpty.dumpty', 'quotes.jok', 'liceprof.sty', 'anime.cli', 'cokeform.txt', 'allfam.epi', 'taping.hum', 'druggame.hum', 'pun.txt', 'critic.txt', 'snapple.rum', 'quux_p.oem', 'pickup.txt', 'hitler.59', 'mundane.v2', 'lost.txt', 'calvin.txt', 'church.sto', 'whatbbs', 'acronym.lis', 'bw.txt', 'basehead.txt', 'killself.hum', 'defectiv.hum', 'freshman.hum', 'ateam.epi', 'english.txt', 'macsfarm.old', 'novel.hum', 'booze.fun', 'donut.txt', 'rinaldo.jok', 'prac1.jok', 'comic_st.gui', 'mitch.txt', 'robot.tes', 'fwksfun.hum', 'cabbage.txt', 'investi.hum', 'grommet.hum', 'grospoem.txt', 'proudlyserve.txt', 'recipe.010', 'gd_hhead.txt', 'trekfume.txt', 'modstup', 'sawyer.txt', 'sw_err.txt', 'epi_tton.txt', 'fearcola.hum', 'weights.hum', 'nurds.hum', 'soporifi.abs', 'jon.txt', 'iced.tea', 'hangover.txt', 'a-team', 'coke.txt', 'insult.lst', 'oliver02.txt', 'rns_ency.txt', 'christop.int', 'lll.hum', 'silverclaws.txt', 'corporat.txt', 'curse.txt', 'epi_bnb.txt', 'parabl.hum', 'religion.txt', 'aniherb.txt', 'candy.txt', 'oldtime.txt', 'fusion.gal', 'btscke01.des', 'just2', 'epi_merm.txt', 'wisdom', 'thievco.txt', 'washroom.txt', 'stereo.txt', 'dead4.txt', 'fartinfo.txt', 'rockmus.hum', 'letgosh.txt', 'hotnnot.hum', 'cowexplo.hum', 'chinese.txt', 'parsnip.txt', 'reeves.txt', 'murphys.txt', 'kanalx.txt', 'ghostsch.hum', 'tnd.1', 'consp.txt', 'chili.txt', 'looser.hum', 'all_grai', 'horflick.txt', 'relative.ada', 'top10st2.txt', 'barney.txt', 'ads.txt', 'tpquotes.txt', 'appbred.brd', 'b12.txt', 'hell.jok', 'chung.iv', 'how.bugs.breakd', 'doggun.sto', 'tfepisod.hum', 'tickmoon.hum', 'number', 'beer.hum', 'epiquest.txt', 'dalive', 'devils.jok', 'tarot.txt', 'exidy.txt', 'boe.hum', 'blackadd', 'libraway.txt', 'curry.txt', 'drinkrul.jok', 'women.jok', 'english', 'pasta001.sal', 'blooprs1.asc', 'test.jok', 'math.2', 'aids.txt', 'smurfs.cc', 'att.txt', 'netmask.txt', 'top10st1.txt', 'oilgluts.hum', 'shorties.jok', 'anime.lif', 'conan.txt', 'poets.hum', 'barney.cn1', 'jac&tuu.hum', 'msorrow', 'scratchy.txt', 'wagon.hum', 'bbh_intv.txt', 'namm', 'bitnet.txt', 'hop.faq', 'get.drunk.cheap', 'test2.jok', 'raven.hum', 'murphy_l.txt', 'excuses.txt', 'oam-001.txt', 'mtm.hum', 'gd_alf.txt', 'rocking.hum', 'hitchcok.txt', 'pro-fact.hum', 'tshirts.jok', 'beginn.ers', 'bmdn01.txt', 'ohandre.hum', 'exam.50', 'curry.hrb', 'anim_lif.txt', 'arab.dic', 'symbol.hum', 'engrhyme.txt', 'drinks.gui', 'cooplaws', 'yogisays.txt', 'wood', 'initials.rid', 'woodbugs.txt', 'eandb.drx', 'abbott.txt', 'hackmorality.txt', 'meat2.txt', 'eskimo.nel', 'lifeimag.hum', 'cbmatic.hum', 'prac3.jok', 'cucumber.txt', 'televisi.hum', 'letter_f.sch', 'moose.txt', 'nigel.7', 'cform2.txt', 'x-drinks.txt', 'saveface.hum', 'makebeer.hum', 'minn.txt', 'onetotwo.hum', 'brush1.txt', 'nukwaste', 'terrnieg.hum', 'dead5.txt', 'myheart.hum', 'shuimai.txt', 'mydaywss.hum', 'polemom.txt', 'lozerzon.hum', 'imprrisk.hum', 'alabama.txt', 'college.sla', 'jokes.txt', 'letter.txt', 'oxymoron.jok', 'diesmurf.txt', 'penndtch', 'signatur.jok', 'phxbbs-m.txt', 'montpyth.hum', 'telecom.q', 'happyhack.txt', 'luzerzo2.hum', 'skippy.txt', 'how_to_i.pro', 'urban.txt', 'killer.hum', 'variety2.asc', 'valujet.txt', 'quotes.bug', 'missheav.hum', 'trukdeth.txt', 'maecenas.hum', 'throwawa.hum', 'fuck!.txt', 'browneco.hum', 'smurfkil.hum', 'humor9.txt', 'hbo_spec.rev', 'gas.txt', 'rednecks.txt', 'caesardr.sal', 'reconcil.hum', 'quack26.txt', 'vonthomp', 'jerky.rcp', 'nigel.6', 'dym', 'wrdnws4.txt', 'gd_gal.txt', 'engineer.hum', 'kilsmur.hum', 'mrscienc.hum', 'nosuch_nasfic', 'twinpeak.txt', 'flux_fix.txt', 'skippy.hum', 'adrian_e.faq', 'xibovac.txt', 'cuchy.hum', 'coladrik.txt', 'rinaldos.law', 'how2bgod.txt', 'newcoke.txt', 'pepper.txt', 'bugbreak.hum', 'cold.fus', 'st_silic.txt', 'subrdead.hum', 'jawgumbo.fis', 'memory.hum', 'beer.txt', 'languag.jok', 'iqtest', 'epi_.txt', 'charity.hum', 'smurf-03.txt', 'jrrt.riddle', 'oculis.rcp', 'timetr.hum', 'blackhol.hum', 'epikarat.txt', 'madscrib.hum', 'spydust.hum', 'thecube.hum', 'maxheadr', 'cars.txt', 'excuse.txt', 'sf-zine.pub', 'talebeat.hum', 'breadpud.des', 'oatbran.rec', 'truthlsd.hum', 'art-fart.hum', 'murphy.txt', 'tuna.lab', 'kid_diet.txt', 'quest.hum', 'hack7.txt', 'polly_.new', 'woodbine.txt', 'lifeinfo.hum', 'ppbeer.txt', 'dromes.txt', 'badday.hum', 'epitaph', 'catranch.hum', 'newmex.hum', 'lotsa.jok', 'japrap.hum', 'llong.hum', 'heroic.txt', 'boneles2.txt', 'stone.hum', 'meinkamp.hum', 'fuckyou2.txt', 'anthropo.stu', 'aussie.lng', 'popconc.hum', 'blood.txt', 'modemwld.txt', 'mog-history', 'contract.moo', \"terrmcd'.hum\", 'whoon1st.hum', 'computer.txt', 'texican.dic', 'grail.txt', 'some_hu.mor', 'bread.rcp', 'fartting.txt', 'missdish', 'practica.txt', 'age.txt', 'the_math.hel', 'bimg.prn', 'woolly_m.amm', 't_zone.jok', 'appetiz.rcp', 'peatchp.hum', 'onetoone.hum', 'televisi.txt', 'chunnel.txt', 'nukewar.txt', 'alcohol.hum', 'a_fish_c.apo', 'rns_bcl.txt', 'strine.txt', 'bnbguide.txt', 'gd_guide.txt', 'proof.met', 'oliver.txt', 'episimp2.txt', 'whoops.hum', 'hitchcoc.app', 'elevator.fun', 'enlightenment.txt', 'jokes1.txt', 'record_.gap', 'coffee.faq', 'booze2.fun', 'psych_pr.quo', 'dining.out', 'oldtime.sng', 'beauty.tm', 'nigel10.txt', 'highland.epi', 'testchri.txt', 'jimhood.txt', 'childhoo.jok', 'proposal.jok', 'pracjoke.txt', 'the_ant.txt', 'let.go', 'cucumber.jok', 'nigel.3', 'lipkovits.txt', 'drunk.txt', 'crzycred.lst', 'jayjay.txt', 'ripoffpc.hum', 'three.txt', 'shameonu.hum', 'misc.1', 'dthought.txt', 'allusion', 'brownie.rec', 'gd_ol.txt', 'exylic.txt', 'teevee.hum', 'jokes', 'pickup.lin', 'lines.jok', 'chainltr.txt', 'losers84.hum', 'luggage.hum', 'pepsideg.txt', 'solders.hum', 'econridl.fun', 'bnbeg2.4.txt', 'clancy.txt', 'nigel.5', 'cartwb.son', 'coldfake.hum', 'gd_liqtv.txt', 'prac2.jok', 'woodsmok.txt', 'incarhel.hum', 'crazy.txt', 'lawskool.txt', 'wrdnws7.txt', 'mel.txt', 'lbinter.hum', 't-10.hum', 'marines.hum', 'quick.jok', 'idaho.txt', 'lawsuniv.hum', 'goforth.hum', 'roadpizz.txt', 'jokeju07.txt', 'bugs.txt', 'onan.txt', 'lansing.txt', 'advrtize.txt', 'nuke.hum', 'element.jok', 'b-2.jok', 'mailfrag.hum', 'top10.txt', 'brewing', 'rentals.hum', 'lp-assoc.txt', 'lozeuser.hum', 'nigel.2', 'confucius_say.txt', 'limerick.jok', 'ludeinfo.txt', 'units.mea', 'fireplacein.txt', 'kilroy', 'dead2.txt', 'herb!.hum', 'murph.jok', 'sfmovie.txt', 'outlimit.txt', 'fed.txt', 'yogurt.asc', 'comrevi1.hum', 'stuf11.txt', 'luvstory.txt', 'topten.hum', 'epi_rns.txt', 'smackjok.hum', 'miami.hum', 'packard.txt', 'ghostfun.hum', 'nihgel_8.9', 'collected_quotes.txt', 'outawork.erl', 'mlverb.hum', 'pukeprom.jok', 'billcat.hum', 'modest.hum', 'yuban.txt', 'lawyer.jok', 'aggie.txt', 'apsnet.txt', 'vegan.rcp', 'fish.rec', 'docdict.txt', 'prawblim.hum', 'passage.hum', 'is_story.txt', 'koans.txt', 'waitress.txt', 'cookie.1', 'inlaws1.txt', 'pizzawho.hum', 'catballs.hum', 'worldend.hum', 'wrdnws1.txt', 'sorority.gir', 'dead3.txt', 'bigpic1.hum', 'homermmm.txt', 'nintendo.jok', 'coladrik.fun', 'necropls.txt', 'hamburge.nam', 'socks.drx', 'office.txt', 'ludeinfo.hum', 'kaboom.hum', 'progrs.gph', 'psilaine.hum', 'oasis', 'ratspit.hum', 'jeffie.heh', 'impurmat.hum', 'chickenheadbbs.txt', 'hitlerap.txt', 'phorse.hum', 'climbing.let', 'moore.txt', 'commutin.jok', 'gd_sgrnd.txt', 'classicm.hum', 'socecon.hum', 'filmgoof.txt', 'quantum.phy', 'adcopy.hum', 'firstaid.txt', 'number.killer', 'quotes.txt', 'food', 'paddingurpapers.txt', 'news.hum', 'justify', 'm0dzmen.hum', 'bw-summe.hat', 'miranda.hum', 'beesherb.txt', 'curiousgeorgie.txt', 'swearfrn.hum', 'phunatdi.ana', 'c0dez.txt', 'coffeebeerwomen.txt', 'radexposed.txt', 'hitler.txt', 'o-ttalk.hum', 'insure.hum', 'free-cof.fee', 'bless.bc', 'sigs.txt', 'beave.hum', 'standard.hum', 'cultmov.faq', 'gd_tznew.txt', 'poopie.txt', 'texican.lex', 'oldeng.hum', 'turing.shr', 'laws.txt', 'insults1.txt', 'arnold.txt', 'beer-gui', 'wetdream.hum', 'chickens.jok', 'shrink.news', 'one.par', 'lifeonledge.txt', 'rns_bwl.txt', 'racist.net', 'coyote.txt', 'manners.txt', 'terbear.txt', 'q.pun', 'social.hum', 'misery.hum', 'wrdnws3.txt', 'alflog.txt', 'smurf_co.txt', 'enquire.hum', 'yjohncse.hum', 'deep.txt', 'reddye.hum', 'pat.txt', 'kashrut.txt', 'hack', 'foodtips', 'ukunderg.txt', 'parades.hum', 'takenote.jok', 'gd_ql.txt', 'shuttleb.hum', 'gohome.hum'}\n"
          ]
        }
      ]
    }
  ]
}